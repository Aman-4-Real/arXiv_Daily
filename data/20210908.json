{"0": {"title": "Title:Effective user intent mining with unsupervised word representation models and topic modelling", "authors": "Authors:Bencheng Wei", "submitted_date": "[Submitted on 4 Sep 2021]", "abstract": "Abstract:  Understanding the intent behind chat between customers and customer service\nagents has become a crucial problem nowadays due to an exponential increase in\nthe use of the Internet by people from different cultures and educational\nbackgrounds. More importantly, the explosion of e-commerce has led to a\nsignificant increase in text conversation between customers and agents. In this\npaper, we propose an approach to data mining the conversation intents behind\nthe textual data. Using the customer service data set, we train unsupervised\ntext representation models, and then develop an intent mapping model which\nwould rank the predefined intents base on cosine similarity between sentences\nand intents. Topic-modeling techniques are used to define intents and domain\nexperts are also involved to interpret topic modelling results. With this\napproach, we can get a good understanding of the user intentions behind the\nunlabelled customer service textual data.", "pdf_link": "https://arxiv.org/pdf/2109.01765"}, "1": {"title": "Title:Hybrid Contrastive Learning of Tri-Modal Representation for Multimodal Sentiment Analysis", "authors": "Authors:Sijie Mai, Ying Zeng, Shuangjia Zheng, Haifeng Hu", "submitted_date": "[Submitted on 4 Sep 2021]", "abstract": "Abstract:  The wide application of smart devices enables the availability of multimodal\ndata, which can be utilized in many tasks. In the field of multimodal sentiment\nanalysis (MSA), most previous works focus on exploring intra- and inter-modal\ninteractions. However, training a network with cross-modal information\n(language, visual, audio) is still challenging due to the modality gap, and\nexisting methods still cannot ensure to sufficiently learn intra-/inter-modal\ndynamics. Besides, while learning dynamics within each sample draws great\nattention, the learning of inter-class relationships is neglected. Moreover,\nthe size of datasets limits the generalization ability of existing methods. To\naddress the afore-mentioned issues, we propose a novel framework HyCon for\nhybrid contrastive learning of tri-modal representation. Specifically, we\nsimultaneously perform intra-/inter-modal contrastive learning and\nsemi-contrastive learning (that is why we call it hybrid contrastive learning),\nwith which the model can fully explore cross-modal interactions, preserve\ninter-class relationships and reduce the modality gap. Besides, a refinement\nterm is devised to prevent the model falling into a sub-optimal solution.\nMoreover, HyCon can naturally generate a large amount of training pairs for\nbetter generalization and reduce the negative effect of limited datasets.\nExtensive experiments on public datasets demonstrate that our proposed method\noutperforms existing works.", "pdf_link": "https://arxiv.org/pdf/2109.01797"}, "2": {"title": "Title:Multi-modal Program Inference: a Marriage of Pre-trainedLanguage Models and Component-based Synthesis", "authors": "Authors:Kia Rahmani, Mohammad Raza, Sumit Gulwani, Vu Le, Daniel Morris, Arjun Radhakrishna, Gustavo Soares, Ashish Tiwari", "submitted_date": "[Submitted on 3 Sep 2021]", "abstract": "Abstract:  Multi-modal program synthesis refers to the task of synthesizing programs\n(code) from their specification given in different forms, such as a combination\nof natural language and examples. Examples provide a precise but incomplete\nspecification, and natural language provides an ambiguous but more \"complete\"\ntask description. Machine-learned pre-trained models (PTMs) are adept at\nhandling ambiguous natural language, but struggle with generating syntactically\nand semantically precise code. Program synthesis techniques can generate\ncorrect code, often even from incomplete but precise specifications, such as\nexamples, but they are unable to work with the ambiguity of natural languages.\nWe present an approach that combines PTMs with component-based synthesis (CBS):\nPTMs are used to generate candidates programs from the natural language\ndescription of the task, which are then used to guide the CBS procedure to find\nthe program that matches the precise examples-based specification. We use our\ncombination approach to instantiate multi-modal synthesis systems for two\nprogramming domains: the domain of regular expressions and the domain of CSS\nselectors. Our evaluation demonstrates the effectiveness of our domain-agnostic\napproach in comparison to a state-of-the-art specialized system, and the\ngenerality of our approach in providing multi-modal program synthesis from\nnatural language and examples in different programming domains.", "pdf_link": "https://arxiv.org/pdf/2109.02445"}, "3": {"title": "Title:Error Detection in Large-Scale Natural Language Understanding Systems Using Transformer Models", "authors": "Authors:Rakesh Chada, Pradeep Natarajan, Darshan Fofadiya, Prathap Ramachandra", "submitted_date": "[Submitted on 4 Sep 2021]", "abstract": "Abstract:  Large-scale conversational assistants like Alexa, Siri, Cortana and Google\nAssistant process every utterance using multiple models for domain, intent and\nnamed entity recognition. Given the decoupled nature of model development and\nlarge traffic volumes, it is extremely difficult to identify utterances\nprocessed erroneously by such systems. We address this challenge to detect\ndomain classification errors using offline Transformer models. We combine\nutterance encodings from a RoBERTa model with the Nbest hypothesis produced by\nthe production system. We then fine-tune end-to-end in a multitask setting\nusing a small dataset of humanannotated utterances with domain classification\nerrors. We tested our approach for detecting misclassifications from one domain\nthat accounts for <0.5% of the traffic in a large-scale conversational AI\nsystem. Our approach achieves an F1 score of 30% outperforming a bi- LSTM\nbaseline by 16.9% and a standalone RoBERTa model by 4.8%. We improve this\nfurther by 2.2% to 32.2% by ensembling multiple models.", "pdf_link": "https://arxiv.org/pdf/2109.01754"}, "4": {"title": "Title:Improving Joint Learning of Chest X-Ray and Radiology Report by Word Region Alignment", "authors": "Authors:Zhanghexuan Ji, Mohammad Abuzar Shaikh, Dana Moukheiber, Sargur Srihari, Yifan Peng, Mingchen Gao", "submitted_date": "[Submitted on 4 Sep 2021]", "abstract": "Abstract:  Self-supervised learning provides an opportunity to explore unlabeled chest\nX-rays and their associated free-text reports accumulated in clinical routine\nwithout manual supervision. This paper proposes a Joint Image Text\nRepresentation Learning Network (JoImTeRNet) for pre-training on chest X-ray\nimages and their radiology reports. The model was pre-trained on both the\nglobal image-sentence level and the local image region-word level for\nvisual-textual matching. Both are bidirectionally constrained on Cross-Entropy\nbased and ranking-based Triplet Matching Losses. The region-word matching is\ncalculated using the attention mechanism without direct supervision about their\nmapping. The pre-trained multi-modal representation learning paves the way for\ndownstream tasks concerning image and/or text encoding. We demonstrate the\nrepresentation learning quality by cross-modality retrievals and multi-label\nclassifications on two datasets: OpenI-IU and MIMIC-CXR", "pdf_link": "https://arxiv.org/pdf/2109.01949"}, "5": {"title": "Title:Multi-Agent Variational Occlusion Inference Using People as Sensors", "authors": "Authors:Masha Itkina, Ye-Ji Mun, Katherine Driggs-Campbell, Mykel J. Kochenderfer", "submitted_date": "[Submitted on 5 Sep 2021]", "abstract": "Abstract:  Autonomous vehicles must reason about spatial occlusions in urban\nenvironments to ensure safety without being overly cautious. Prior work\nexplored occlusion inference from observed social behaviors of road agents.\nInferring occupancy from agent behaviors is an inherently multimodal problem; a\ndriver may behave in the same manner for different occupancy patterns ahead of\nthem (e.g., a driver may move at constant speed in traffic or on an open road).\nPast work, however, does not account for this multimodality, thus neglecting to\nmodel this source of aleatoric uncertainty in the relationship between driver\nbehaviors and their environment. We propose an occlusion inference method that\ncharacterizes observed behaviors of human agents as sensor measurements, and\nfuses them with those from a standard sensor suite. To capture the aleatoric\nuncertainty, we train a conditional variational autoencoder with a discrete\nlatent space to learn a multimodal mapping from observed driver trajectories to\nan occupancy grid representation of the view ahead of the driver. Our method\nhandles multi-agent scenarios, combining measurements from multiple observed\ndrivers using evidential theory to solve the sensor fusion problem. Our\napproach is validated on a real-world dataset, outperforming baselines and\ndemonstrating real-time capable performance. Our code is available at\nthis https URL .", "pdf_link": "https://arxiv.org/pdf/2109.02173"}, "6": {"title": "Title:Learning-Based Strategy Design for Robot-Assisted Reminiscence Therapy Based on a Developed Model for People with Dementia", "authors": "Authors:Fengpei Yuan, Ran Zhang, Dania Bilal, Xiaopeng Zhao", "submitted_date": "[Submitted on 6 Sep 2021]", "abstract": "Abstract:  In this paper, the robot-assisted Reminiscence Therapy (RT) is studied as a\npsychosocial intervention to persons with dementia (PwDs). We aim at a\nconversation strategy for the robot by reinforcement learning to stimulate the\nPwD to talk. Specifically, to characterize the stochastic reactions of a PwD to\nthe robot's actions, a simulation model of a PwD is developed which features\nthe transition probabilities among different PwD states consisting of the\nresponse relevance, emotion levels and confusion conditions. A Q-learning (QL)\nalgorithm is then designed to achieve the best conversation strategy for the\nrobot. The objective is to stimulate the PwD to talk as much as possible while\nkeeping the PwD's states as positive as possible. In certain conditions, the\nachieved strategy gives the PwD choices to continue or change the topic, or\nstop the conversation, so that the PwD has a sense of control to mitigate the\nconversation stress. To achieve this, the standard QL algorithm is revised to\ndeliberately integrate the impact of PwD's choices into the Q-value updates.\nFinally, the simulation results demonstrate the learning convergence and\nvalidate the efficacy of the achieved strategy. Tests show that the strategy is\ncapable to duly adjust the difficulty level of prompt according to the PwD's\nstates, take actions (e.g., repeat or explain the prompt, or comfort) to help\nthe PwD out of bad states, and allow the PwD to control the conversation\ntendency when bad states continue.", "pdf_link": "https://arxiv.org/pdf/2109.02194"}, "7": {"title": "Title:MONITOR: A Multimodal Fusion Framework to Assess Message Veracity in Social Networks", "authors": "Authors:Abderrazek Azri (ERIC), C\u00e9cile Favre (ERIC), Nouria Harbi (ERIC), J\u00e9r\u00f4me Darmont (ERIC), Camille No\u00fbs", "submitted_date": "[Submitted on 6 Sep 2021]", "abstract": "Abstract:  Users of social networks tend to post and share content with little\nrestraint. Hence, rumors and fake news can quickly spread on a huge scale. This\nmay pose a threat to the credibility of social media and can cause serious\nconsequences in real life. Therefore, the task of rumor detection and\nverification has become extremely important. Assessing the veracity of a social\nmedia message (e.g., by fact checkers) involves analyzing the text of the\nmessage, its context and any multimedia attachment. This is a very\ntime-consuming task that can be much helped by machine learning. In the\nliterature, most message veracity verification methods only exploit textual\ncontents and metadata. Very few take both textual and visual contents, and more\nparticularly images, into account. In this paper, we second the hypothesis that\nexploiting all of the components of a social media post enhances the accuracy\nof veracity detection. To further the state of the art, we first propose using\na set of advanced image features that are inspired from the field of image\nquality assessment, which effectively contributes to rumor detection. These\nmetrics are good indicators for the detection of fake images, even for those\ngenerated by advanced techniques like generative adversarial networks (GANs).\nThen, we introduce the Multimodal fusiON framework to assess message veracIty\nin social neTwORks (MONITOR), which exploits all message features (i.e., text,\nsocial context, and image features) by supervised machine learning. Such\nalgorithms provide interpretability and explainability in the decisions taken,\nwhich we believe is particularly important in the context of rumor\nverification. Experimental results show that MONITOR can detect rumors with an\naccuracy of 96% and 89% on the MediaEval benchmark and the FakeNewsNet dataset,\nrespectively. These results are significantly better than those of\nstate-of-the-art machine learning baselines.", "pdf_link": "https://arxiv.org/pdf/2109.02271"}, "8": {"title": "Title:Enhancing Visual Dialog Questioner with Entity-based Strategy Learning and Augmented Guesser", "authors": "Authors:Duo Zheng, Zipeng Xu, Fandong Meng, Xiaojie Wang, Jiaan Wang, Jie Zhou", "submitted_date": "[Submitted on 6 Sep 2021]", "abstract": "Abstract:  Considering the importance of building a good Visual Dialog (VD) Questioner,\nmany researchers study the topic under a Q-Bot-A-Bot image-guessing game\nsetting, where the Questioner needs to raise a series of questions to collect\ninformation of an undisclosed image. Despite progress has been made in\nSupervised Learning (SL) and Reinforcement Learning (RL), issues still exist.\nFirstly, previous methods do not provide explicit and effective guidance for\nQuestioner to generate visually related and informative questions. Secondly,\nthe effect of RL is hampered by an incompetent component, i.e., the Guesser,\nwho makes image predictions based on the generated dialogs and assigns rewards\naccordingly. To enhance VD Questioner: 1) we propose a Related entity enhanced\nQuestioner (ReeQ) that generates questions under the guidance of related\nentities and learns entity-based questioning strategy from human dialogs; 2) we\npropose an Augmented Guesser (AugG) that is strong and is optimized for the VD\nsetting especially. Experimental results on the VisDial v1.0 dataset show that\nour approach achieves state-of-theart performance on both image-guessing task\nand question diversity. Human study further proves that our model generates\nmore visually related, informative and coherent questions.", "pdf_link": "https://arxiv.org/pdf/2109.02297"}, "9": {"title": "Title:Proto: A Neural Cocktail for Generating Appealing Conversations", "authors": "Authors:Sougata Saha, Souvik Das, Elizabeth Soper, Erin Pacquetet, Rohini K. Srihari", "submitted_date": "[Submitted on 6 Sep 2021]", "abstract": "Abstract:  In this paper, we present our Alexa Prize Grand Challenge 4 socialbot: Proto.\nLeveraging diverse sources of world knowledge, and powered by a suite of neural\nand rule-based natural language understanding modules, state-of-the-art neural\ngenerators, novel state-based deterministic generators, an ensemble of neural\nre-rankers, a robust post-processing algorithm, and an efficient overall\nconversation strategy, Proto strives to be able to converse coherently about a\ndiverse range of topics of interest to humans, and provide a memorable\nexperience to the user. In this paper we dissect and analyze the different\ncomponents and conversation strategies implemented by our socialbot, which\nenables us to generate colloquial, empathetic, engaging, self-rectifying,\nfactually correct, and on-topic response, which has helped us achieve\nconsistent scores throughout the competition.", "pdf_link": "https://arxiv.org/pdf/2109.02513"}, "10": {"title": "Title:Self-recognition in conversational agents", "authors": "Authors:Yigit Oktar, Erdem Okur, Mehmet Turkan", "submitted_date": "[Submitted on 6 Feb 2020 (v1), last revised 5 Sep 2021 (this version, v3)]", "abstract": "Abstract:  In a standard Turing test, a machine has to prove its humanness to the\njudges. By successfully imitating a thinking entity such as a human, this\nmachine then proves that it can also think. Some objections claim that Turing\ntest is not a tool to demonstrate the existence of general intelligence or\nthinking activity. A compelling alternative is the Lovelace test, in which the\nagent must originate a product that the agent's creator cannot explain.\nTherefore, the agent must be the owner of an original product. However, for\nthis to happen the agent must exhibit the idea of self and distinguish oneself\nfrom others. Sustaining the idea of self within the Turing test is still\npossible if the judge decides to act as a textual mirror. Self-recognition\ntests applied on animals through mirrors appear to be viable tools to\ndemonstrate the existence of a type of general intelligence. Methodology here\nconstructs a textual version of the mirror test by placing the agent as the one\nand only judge to figure out whether the contacted one is an other, a mimicker,\nor oneself in an unsupervised manner. This textual version of the mirror test\nis objective, self-contained, and devoid of humanness. Any agent passing this\ntextual mirror test should have or can acquire a thought mechanism that can be\nreferred to as the inner-voice, answering the original and long lasting\nquestion of Turing \"Can machines think?\" in a constructive manner still within\nthe bounds of the Turing test. Moreover, it is possible that a successful\nself-recognition might pave way to stronger notions of self-awareness in\nartificial beings.", "pdf_link": "https://arxiv.org/pdf/2002.02334"}, "11": {"title": "Title:CX-ToM: Counterfactual Explanations with Theory-of-Mind for Enhancing Human Trust in Image Recognition Models", "authors": "Authors:Arjun R. Akula, Keze Wang, Changsong Liu, Sari Saba-Sadiya, Hongjing Lu, Sinisa Todorovic, Joyce Chai, Song-Chun Zhu", "submitted_date": "[Submitted on 3 Sep 2021 (v1), last revised 6 Sep 2021 (this version, v2)]", "abstract": "Abstract:  We propose CX-ToM, short for counterfactual explanations with theory-of mind,\na new explainable AI (XAI) framework for explaining decisions made by a deep\nconvolutional neural network (CNN). In contrast to the current methods in XAI\nthat generate explanations as a single shot response, we pose explanation as an\niterative communication process, i.e. dialog, between the machine and human\nuser. More concretely, our CX-ToM framework generates sequence of explanations\nin a dialog by mediating the differences between the minds of machine and human\nuser. To do this, we use Theory of Mind (ToM) which helps us in explicitly\nmodeling human's intention, machine's mind as inferred by the human as well as\nhuman's mind as inferred by the machine. Moreover, most state-of-the-art XAI\nframeworks provide attention (or heat map) based explanations. In our work, we\nshow that these attention based explanations are not sufficient for increasing\nhuman trust in the underlying CNN model. In CX-ToM, we instead use\ncounterfactual explanations called fault-lines which we define as follows:\ngiven an input image I for which a CNN classification model M predicts class\nc_pred, a fault-line identifies the minimal semantic-level features (e.g.,\nstripes on zebra, pointed ears of dog), referred to as explainable concepts,\nthat need to be added to or deleted from I in order to alter the classification\ncategory of I by M to another specified class c_alt. We argue that, due to the\niterative, conceptual and counterfactual nature of CX-ToM explanations, our\nframework is practical and more natural for both expert and non-expert users to\nunderstand the internal workings of complex deep learning models. Extensive\nquantitative and qualitative experiments verify our hypotheses, demonstrating\nthat our CX-ToM significantly outperforms the state-of-the-art explainable AI\nmodels.", "pdf_link": "https://arxiv.org/pdf/2109.01401"}, "12": {"title": "Title:Migratable AI : Investigating users' affect on identity and information migration of a conversational AI agent", "authors": "Authors:Ravi Tejwani, Boris Katz, Cynthia Breazeal", "submitted_date": "[Submitted on 22 Oct 2020 (v1), last revised 4 Sep 2021 (this version, v2)]", "abstract": "Abstract:  Conversational AI agents are becoming ubiquitous and provide assistance to us\nin our everyday activities. In recent years, researchers have explored the\nmigration of these agents across different embodiments in order to maintain the\ncontinuity of the task and improve user experience. In this paper, we\ninvestigate user's affective responses in different configurations of the\nmigration parameters. We present a 2x2 between-subjects study in a task-based\nscenario using information migration and identity migration as parameters. We\noutline the affect processing pipeline from the video footage collected during\nthe study and report user's responses in each condition. Our results show that\nusers reported highest joy and were most surprised when both the information\nand identity was migrated; and reported most anger when the information was\nmigrated without the identity of their agent.", "pdf_link": "https://arxiv.org/pdf/2010.13319"}, "13": {"title": "Title:Multimodal Reward Shaping for Efficient Exploration in Reinforcement Learning", "authors": "Authors:Mingqi Yuan, Mon-on Pun, Dong Wang, Yi Chen, Haojun Li", "submitted_date": "[Submitted on 19 Jul 2021 (v1), last revised 6 Sep 2021 (this version, v2)]", "abstract": "Abstract:  Maintaining the long-term exploration capability of the agent remains one of\nthe critical challenges in deep reinforcement learning. A representative\nsolution is to leverage reward shaping to provide intrinsic rewards for the\nagent to encourage exploration. However, most existing methods suffer from\nvanishing intrinsic rewards, which cannot provide sustainable exploration\nincentives. Moreover, they rely heavily on complex models and additional memory\nto record learning procedures, resulting in high computational complexity and\nlow robustness. To tackle this problem, entropy-based methods are proposed to\nevaluate the global exploration performance, encouraging the agent to visit the\nstate space more equitably. However, the sample complexity of estimating the\nstate visitation entropy is prohibitive when handling environments with\nhigh-dimensional observations. In this paper, we introduce a novel metric\nentitled Jain's fairness index (JFI) to replace the entropy regularizer, which\nsolves the exploration problem from a brand new perspective. In sharp contrast\nto the entropy regularizer, JFI is more computable and robust and can be easily\napplied generalized into arbitrary tasks. Furthermore, we leverage a\nvariational auto-encoder (VAE) model to capture the life-long novelty of\nstates, which is combined with the global JFI score to form multimodal\nintrinsic rewards. Finally, extensive simulation results demonstrate that our\nmultimodal reward shaping (MMRS) method can achieve higher performance than\nother benchmark schemes.", "pdf_link": "https://arxiv.org/pdf/2107.08888"}, "14": {"title": "Title:M^2-MedDialog: A Dataset and Benchmarks for Multi-domain Multi-service Medical Dialogues", "authors": "Authors:Guojun Yan, Jiahuan Pei, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Huasheng Liang", "submitted_date": "[Submitted on 1 Sep 2021 (v1), last revised 6 Sep 2021 (this version, v2)]", "abstract": "Abstract:  Medical dialogue systems (MDSs) aim to assist doctors and patients with a\nrange of professional medical services, i.e., diagnosis, consultation, and\ntreatment. However, one-stop MDS is still unexplored because: (1) no dataset\nhas so large-scale dialogues contains both multiple medical services and\nfine-grained medical labels (i.e., intents, slots, values); (2) no model has\naddressed a MDS based on multiple-service conversations in a unified framework.\nIn this work, we first build a Multiple-domain Multiple-service medical\ndialogue (M^2-MedDialog)dataset, which contains 1,557 conversations between\ndoctors and patients, covering 276 types of diseases, 2,468 medical entities,\nand 3 specialties of medical services. To the best of our knowledge, it is the\nonly medical dialogue dataset that includes both multiple medical services and\nfine-grained medical labels. Then, we formulate a one-stop MDS as a\nsequence-to-sequence generation problem. We unify a MDS with causal language\nmodeling and conditional causal language modeling, respectively. Specifically,\nwe employ several pretrained models (i.e., BERT-WWM, BERT-MED, GPT2, and MT5)\nand their variants to get benchmarks on M^2-MedDialog dataset. We also propose\npseudo labeling and natural perturbation methods to expand M2-MedDialog dataset\nand enhance the state-of-the-art pretrained models. We demonstrate the results\nachieved by the benchmarks so far through extensive experiments on\nM2-MedDialog. We release the dataset, the code, as well as the evaluation\nscripts to facilitate future research in this important research direction.", "pdf_link": "https://arxiv.org/pdf/2109.00430"}, "15": {"title": "Title:Error Detection in Large-Scale Natural Language Understanding Systems Using Transformer Models", "authors": "Authors:Rakesh Chada, Pradeep Natarajan, Darshan Fofadiya, Prathap Ramachandra", "submitted_date": "[Submitted on 4 Sep 2021]", "abstract": "Abstract:  Large-scale conversational assistants like Alexa, Siri, Cortana and Google\nAssistant process every utterance using multiple models for domain, intent and\nnamed entity recognition. Given the decoupled nature of model development and\nlarge traffic volumes, it is extremely difficult to identify utterances\nprocessed erroneously by such systems. We address this challenge to detect\ndomain classification errors using offline Transformer models. We combine\nutterance encodings from a RoBERTa model with the Nbest hypothesis produced by\nthe production system. We then fine-tune end-to-end in a multitask setting\nusing a small dataset of humanannotated utterances with domain classification\nerrors. We tested our approach for detecting misclassifications from one domain\nthat accounts for <0.5% of the traffic in a large-scale conversational AI\nsystem. Our approach achieves an F1 score of 30% outperforming a bi- LSTM\nbaseline by 16.9% and a standalone RoBERTa model by 4.8%. We improve this\nfurther by 2.2% to 32.2% by ensembling multiple models.", "pdf_link": "https://arxiv.org/pdf/2109.01754"}, "16": {"title": "Title:Towards Expressive Communication with Internet Memes: A New Multimodal Conversation Dataset and Benchmark", "authors": "Authors:Zhengcong Fei, Zekang Li, Jinchao Zhang, Yang Feng, Jie Zhou", "submitted_date": "[Submitted on 4 Sep 2021]", "abstract": "Abstract:  As a kind of new expression elements, Internet memes are popular and\nextensively used in online chatting scenarios since they manage to make\ndialogues vivid, moving, and interesting. However, most current dialogue\nresearches focus on text-only dialogue tasks. In this paper, we propose a new\ntask named as \\textbf{M}eme incorporated \\textbf{O}pen-domain \\textbf{D}ialogue\n(MOD). Compared to previous dialogue tasks, MOD is much more challenging since\nit requires the model to understand the multimodal elements as well as the\nemotions behind them. To facilitate the MOD research, we construct a\nlarge-scale open-domain multimodal dialogue dataset incorporating abundant\nInternet memes into utterances. The dataset consists of $\\sim$45K Chinese\nconversations with $\\sim$606K utterances. Each conversation contains about $13$\nutterances with about $4$ Internet memes on average and each utterance equipped\nwith an Internet meme is annotated with the corresponding emotion. In addition,\nwe present a simple and effective method, which utilizes a unified generation\nnetwork to solve the MOD task. Experimental results demonstrate that our method\ntrained on the proposed corpus is able to achieve expressive communication\nincluding texts and memes. The corpus and models have been publicly available\nat this https URL.", "pdf_link": "https://arxiv.org/pdf/2109.01839"}, "17": {"title": "Title:Supervised Contrastive Learning for Multimodal Unreliable News Detection in COVID-19 Pandemic", "authors": "Authors:Wenjia Zhang, Lin Gui, Yulan He", "submitted_date": "[Submitted on 4 Sep 2021]", "abstract": "Abstract:  As the digital news industry becomes the main channel of information\ndissemination, the adverse impact of fake news is explosively magnified. The\ncredibility of a news report should not be considered in isolation. Rather,\npreviously published news articles on the similar event could be used to assess\nthe credibility of a news report. Inspired by this, we propose a BERT-based\nmultimodal unreliable news detection framework, which captures both textual and\nvisual information from unreliable articles utilising the contrastive learning\nstrategy. The contrastive learner interacts with the unreliable news classifier\nto push similar credible news (or similar unreliable news) closer while moving\nnews articles with similar content but opposite credibility labels away from\neach other in the multimodal embedding space. Experimental results on a\nCOVID-19 related dataset, ReCOVery, show that our model outperforms a number of\ncompetitive baseline in unreliable news detection.", "pdf_link": "https://arxiv.org/pdf/2109.01850"}, "18": {"title": "Title:A Neural Network-Based Linguistic Similarity Measure for Entrainment in Conversations", "authors": "Authors:Mingzhi Yu, Diane Litman, Shuang Ma, Jian Wu", "submitted_date": "[Submitted on 4 Sep 2021]", "abstract": "Abstract:  Linguistic entrainment is a phenomenon where people tend to mimic each other\nin conversation. The core instrument to quantify entrainment is a linguistic\nsimilarity measure between conversational partners. Most of the current\nsimilarity measures are based on bag-of-words approaches that rely on\nlinguistic markers, ignoring the overall language structure and dialogue\ncontext. To address this issue, we propose to use a neural network model to\nperform the similarity measure for entrainment. Our model is context-aware, and\nit further leverages a novel component to learn the shared high-level\nlinguistic features across dialogues. We first investigate the effectiveness of\nour novel component. Then we use the model to perform similarity measure in a\ncorpus-based entrainment analysis. We observe promising results for both\nevaluation tasks.", "pdf_link": "https://arxiv.org/pdf/2109.01924"}, "19": {"title": "Title:SideControl: Controlled Open-domain Dialogue Generation via Additive Side Networks", "authors": "Authors:Wanyu Du, Yangfeng Ji", "submitted_date": "[Submitted on 5 Sep 2021]", "abstract": "Abstract:  Transformer-based pre-trained language models boost the performance of\nopen-domain dialogue systems. Prior works leverage Transformer-based\npre-trained language models to generate texts with desired attributes in two\ngeneral approaches: (1) gradient-based methods: updating all latent\nrepresentations of pre-trained models with gradients from attribute models; (2)\nweighted-decoding methods: re-ranking beam candidates from pre-trained models\nwith attribute functions. However, gradient-based methods lead to high\ncomputation cost and can easily get overfitted on small training sets, while\nweighted-decoding methods are inherently constrained by the low-variance\nhigh-bias pre-trained model. In this work, we propose a novel approach to\ncontrol the generation of Transformer-based pre-trained language models: the\nSideControl framework, which leverages a novel control attributes loss to\nincorporate useful control signals, and is shown to perform well with very\nlimited training samples. We evaluate our proposed method on two benchmark\nopen-domain dialogue datasets, and results show that the SideControl framework\nhas better controllability, higher generation quality and better\nsample-efficiency than existing gradient-based and weighted-decoding baselines.", "pdf_link": "https://arxiv.org/pdf/2109.01958"}, "20": {"title": "Title:Re-entry Prediction for Online Conversations via Self-Supervised Learning", "authors": "Authors:Lingzhi Wang, Xingshan Zeng, Huang Hu, Kam-Fai Wong, Daxin Jiang", "submitted_date": "[Submitted on 5 Sep 2021]", "abstract": "Abstract:  In recent years, world business in online discussions and opinion sharing on\nsocial media is booming. Re-entry prediction task is thus proposed to help\npeople keep track of the discussions which they wish to continue. Nevertheless,\nexisting works only focus on exploiting chatting history and context\ninformation, and ignore the potential useful learning signals underlying\nconversation data, such as conversation thread patterns and repeated engagement\nof target users, which help better understand the behavior of target users in\nconversations. In this paper, we propose three interesting and well-founded\nauxiliary tasks, namely, Spread Pattern, Repeated Target user, and Turn\nAuthorship, as the self-supervised signals for re-entry prediction. These\nauxiliary tasks are trained together with the main task in a multi-task manner.\nExperimental results on two datasets newly collected from Twitter and Reddit\nshow that our method outperforms the previous state-of-the-arts with fewer\nparameters and faster convergence. Extensive experiments and analysis show the\neffectiveness of our proposed models and also point out some key ideas in\ndesigning self-supervised tasks.", "pdf_link": "https://arxiv.org/pdf/2109.02020"}, "21": {"title": "Title:Transformer Models for Text Coherence Assessment", "authors": "Authors:Tushar Abhishek, Daksh Rawat, Manish Gupta, Vasudeva Varma", "submitted_date": "[Submitted on 5 Sep 2021]", "abstract": "Abstract:  Coherence is an important aspect of text quality and is crucial for ensuring\nits readability. It is essential desirable for outputs from text generation\nsystems like summarization, question answering, machine translation, question\ngeneration, table-to-text, etc. An automated coherence scoring model is also\nhelpful in essay scoring or providing writing feedback. A large body of\nprevious work has leveraged entity-based methods, syntactic patterns, discourse\nrelations, and more recently traditional deep learning architectures for text\ncoherence assessment. Previous work suffers from drawbacks like the inability\nto handle long-range dependencies, out-of-vocabulary words, or model sequence\ninformation. We hypothesize that coherence assessment is a cognitively complex\ntask that requires deeper models and can benefit from other related tasks.\nAccordingly, in this paper, we propose four different Transformer-based\narchitectures for the task: vanilla Transformer, hierarchical Transformer,\nmulti-task learning-based model, and a model with fact-based input\nrepresentation. Our experiments with popular benchmark datasets across multiple\ndomains on four different coherence assessment tasks demonstrate that our\nmodels achieve state-of-the-art results outperforming existing models by a good\nmargin.", "pdf_link": "https://arxiv.org/pdf/2109.02176"}, "22": {"title": "Title:Enhancing Visual Dialog Questioner with Entity-based Strategy Learning and Augmented Guesser", "authors": "Authors:Duo Zheng, Zipeng Xu, Fandong Meng, Xiaojie Wang, Jiaan Wang, Jie Zhou", "submitted_date": "[Submitted on 6 Sep 2021]", "abstract": "Abstract:  Considering the importance of building a good Visual Dialog (VD) Questioner,\nmany researchers study the topic under a Q-Bot-A-Bot image-guessing game\nsetting, where the Questioner needs to raise a series of questions to collect\ninformation of an undisclosed image. Despite progress has been made in\nSupervised Learning (SL) and Reinforcement Learning (RL), issues still exist.\nFirstly, previous methods do not provide explicit and effective guidance for\nQuestioner to generate visually related and informative questions. Secondly,\nthe effect of RL is hampered by an incompetent component, i.e., the Guesser,\nwho makes image predictions based on the generated dialogs and assigns rewards\naccordingly. To enhance VD Questioner: 1) we propose a Related entity enhanced\nQuestioner (ReeQ) that generates questions under the guidance of related\nentities and learns entity-based questioning strategy from human dialogs; 2) we\npropose an Augmented Guesser (AugG) that is strong and is optimized for the VD\nsetting especially. Experimental results on the VisDial v1.0 dataset show that\nour approach achieves state-of-theart performance on both image-guessing task\nand question diversity. Human study further proves that our model generates\nmore visually related, informative and coherent questions.", "pdf_link": "https://arxiv.org/pdf/2109.02297"}, "23": {"title": "Title:Vision Guided Generative Pre-trained Language Models for Multimodal Abstractive Summarization", "authors": "Authors:Tiezheng Yu, Wenliang Dai, Zihan Liu, Pascale Fung", "submitted_date": "[Submitted on 6 Sep 2021]", "abstract": "Abstract:  Multimodal abstractive summarization (MAS) models that summarize videos\n(vision modality) and their corresponding transcripts (text modality) are able\nto extract the essential information from massive multimodal data on the\nInternet. Recently, large-scale generative pre-trained language models (GPLMs)\nhave been shown to be effective in text generation tasks. However, existing MAS\nmodels cannot leverage GPLMs' powerful generation ability. To fill this\nresearch gap, we aim to study two research questions: 1) how to inject visual\ninformation into GPLMs without hurting their generation ability; and 2) where\nis the optimal place in GPLMs to inject the visual information? In this paper,\nwe present a simple yet effective method to construct vision guided (VG) GPLMs\nfor the MAS task using attention-based add-on layers to incorporate visual\ninformation while maintaining their original text generation ability. Results\nshow that our best model significantly surpasses the prior state-of-the-art\nmodel by 5.7 ROUGE-1, 5.3 ROUGE-2, and 5.1 ROUGE-L scores on the How2 dataset,\nand our visual guidance method contributes 83.6% of the overall improvement.\nFurthermore, we conduct thorough ablation studies to analyze the effectiveness\nof various modality fusion methods and fusion locations.", "pdf_link": "https://arxiv.org/pdf/2109.02401"}, "24": {"title": "Title:DialogLM: Pre-trained Model for Long Dialogue Understanding and Summarization", "authors": "Authors:Ming Zhong, Yang Liu, Yichong Xu, Chenguang Zhu, Michael Zeng", "submitted_date": "[Submitted on 6 Sep 2021]", "abstract": "Abstract:  Dialogue is an essential part of human communication and cooperation.\nExisting research mainly focuses on short dialogue scenarios in a one-on-one\nfashion. However, multi-person interactions in the real world, such as meetings\nor interviews, are frequently over a few thousand words. There is still a lack\nof corresponding research and powerful tools to understand and process such\nlong dialogues. Therefore, in this work, we present a pre-training framework\nfor long dialogue understanding and summarization. Considering the nature of\nlong conversations, we propose a window-based denoising approach for generative\npre-training. For a dialogue, it corrupts a window of text with\ndialogue-inspired noise, and guides the model to reconstruct this window based\non the content of the remaining conversation. Furthermore, to process longer\ninput, we augment the model with sparse attention which is combined with\nconventional attention in a hybrid manner. We conduct extensive experiments on\nfive datasets of long dialogues, covering tasks of dialogue summarization,\nabstractive question answering and topic segmentation. Experimentally, we show\nthat our pre-trained model DialogLM significantly surpasses the\nstate-of-the-art models across datasets and tasks.", "pdf_link": "https://arxiv.org/pdf/2109.02492"}, "25": {"title": "Title:Proto: A Neural Cocktail for Generating Appealing Conversations", "authors": "Authors:Sougata Saha, Souvik Das, Elizabeth Soper, Erin Pacquetet, Rohini K. Srihari", "submitted_date": "[Submitted on 6 Sep 2021]", "abstract": "Abstract:  In this paper, we present our Alexa Prize Grand Challenge 4 socialbot: Proto.\nLeveraging diverse sources of world knowledge, and powered by a suite of neural\nand rule-based natural language understanding modules, state-of-the-art neural\ngenerators, novel state-based deterministic generators, an ensemble of neural\nre-rankers, a robust post-processing algorithm, and an efficient overall\nconversation strategy, Proto strives to be able to converse coherently about a\ndiverse range of topics of interest to humans, and provide a memorable\nexperience to the user. In this paper we dissect and analyze the different\ncomponents and conversation strategies implemented by our socialbot, which\nenables us to generate colloquial, empathetic, engaging, self-rectifying,\nfactually correct, and on-topic response, which has helped us achieve\nconsistent scores throughout the competition.", "pdf_link": "https://arxiv.org/pdf/2109.02513"}, "26": {"title": "Title:Improving Joint Learning of Chest X-Ray and Radiology Report by Word Region Alignment", "authors": "Authors:Zhanghexuan Ji, Mohammad Abuzar Shaikh, Dana Moukheiber, Sargur Srihari, Yifan Peng, Mingchen Gao", "submitted_date": "[Submitted on 4 Sep 2021]", "abstract": "Abstract:  Self-supervised learning provides an opportunity to explore unlabeled chest\nX-rays and their associated free-text reports accumulated in clinical routine\nwithout manual supervision. This paper proposes a Joint Image Text\nRepresentation Learning Network (JoImTeRNet) for pre-training on chest X-ray\nimages and their radiology reports. The model was pre-trained on both the\nglobal image-sentence level and the local image region-word level for\nvisual-textual matching. Both are bidirectionally constrained on Cross-Entropy\nbased and ranking-based Triplet Matching Losses. The region-word matching is\ncalculated using the attention mechanism without direct supervision about their\nmapping. The pre-trained multi-modal representation learning paves the way for\ndownstream tasks concerning image and/or text encoding. We demonstrate the\nrepresentation learning quality by cross-modality retrievals and multi-label\nclassifications on two datasets: OpenI-IU and MIMIC-CXR", "pdf_link": "https://arxiv.org/pdf/2109.01949"}, "27": {"title": "Title:Emotion Dynamics in Movie Dialogues", "authors": "Authors:Will E. Hipson, Saif M. Mohammad", "submitted_date": "[Submitted on 1 Mar 2021 (v1), last revised 6 Sep 2021 (this version, v5)]", "abstract": "Abstract:  Emotion dynamics is a framework for measuring how an individual's emotions\nchange over time. It is a powerful tool for understanding how we behave and\ninteract with the world. In this paper, we introduce a framework to track\nemotion dynamics through one's utterances. Specifically we introduce a number\nof utterance emotion dynamics (UED) metrics inspired by work in Psychology. We\nuse this approach to trace emotional arcs of movie characters. We analyze\nthousands of such character arcs to test hypotheses that inform our broader\nunderstanding of stories. Notably, we show that there is a tendency for\ncharacters to use increasingly more negative words and become increasingly\nemotionally discordant with each other until about 90 percent of the narrative\nlength. UED also has applications in behavior studies, social sciences, and\npublic health.", "pdf_link": "https://arxiv.org/pdf/2103.01345"}, "28": {"title": "Title:CSDS: A Fine-Grained Chinese Dataset for Customer Service Dialogue Summarization", "authors": "Authors:Haitao Lin (1 and 2), Liqun Ma (1 and 2), Junnan Zhu (1 and 2), Lu Xiang (1 and 2), Yu Zhou (1 and 3), Jiajun Zhang (1 and 2), Chengqing Zong (1 and 2) ((1) National Laboratory of Pattern Recognition, Institute of Automation, CAS, Beijing, China, (2) School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China, (3) Fanyu AI Laboratory, Zhongke Fanyu Technology Co., Ltd, Beijing, China)", "submitted_date": "[Submitted on 30 Aug 2021 (v1), last revised 6 Sep 2021 (this version, v2)]", "abstract": "Abstract:  Dialogue summarization has drawn much attention recently. Especially in the\ncustomer service domain, agents could use dialogue summaries to help boost\ntheir works by quickly knowing customer's issues and service progress. These\napplications require summaries to contain the perspective of a single speaker\nand have a clear topic flow structure, while neither are available in existing\ndatasets. Therefore, in this paper, we introduce a novel Chinese dataset for\nCustomer Service Dialogue Summarization (CSDS). CSDS improves the abstractive\nsummaries in two aspects: (1) In addition to the overall summary for the whole\ndialogue, role-oriented summaries are also provided to acquire different\nspeakers' viewpoints. (2) All the summaries sum up each topic separately, thus\ncontaining the topic-level structure of the dialogue. We define tasks in CSDS\nas generating the overall summary and different role-oriented summaries for a\ngiven dialogue. Next, we compare various summarization methods on CSDS, and\nexperiment results show that existing methods are prone to generate redundant\nand incoherent summaries. Besides, the performance becomes much worse when\nanalyzing the performance on role-oriented summaries and topic structures. We\nhope that this study could benchmark Chinese dialogue summarization and benefit\nfurther studies.", "pdf_link": "https://arxiv.org/pdf/2108.13139"}, "29": {"title": "Title:M^2-MedDialog: A Dataset and Benchmarks for Multi-domain Multi-service Medical Dialogues", "authors": "Authors:Guojun Yan, Jiahuan Pei, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Huasheng Liang", "submitted_date": "[Submitted on 1 Sep 2021 (v1), last revised 6 Sep 2021 (this version, v2)]", "abstract": "Abstract:  Medical dialogue systems (MDSs) aim to assist doctors and patients with a\nrange of professional medical services, i.e., diagnosis, consultation, and\ntreatment. However, one-stop MDS is still unexplored because: (1) no dataset\nhas so large-scale dialogues contains both multiple medical services and\nfine-grained medical labels (i.e., intents, slots, values); (2) no model has\naddressed a MDS based on multiple-service conversations in a unified framework.\nIn this work, we first build a Multiple-domain Multiple-service medical\ndialogue (M^2-MedDialog)dataset, which contains 1,557 conversations between\ndoctors and patients, covering 276 types of diseases, 2,468 medical entities,\nand 3 specialties of medical services. To the best of our knowledge, it is the\nonly medical dialogue dataset that includes both multiple medical services and\nfine-grained medical labels. Then, we formulate a one-stop MDS as a\nsequence-to-sequence generation problem. We unify a MDS with causal language\nmodeling and conditional causal language modeling, respectively. Specifically,\nwe employ several pretrained models (i.e., BERT-WWM, BERT-MED, GPT2, and MT5)\nand their variants to get benchmarks on M^2-MedDialog dataset. We also propose\npseudo labeling and natural perturbation methods to expand M2-MedDialog dataset\nand enhance the state-of-the-art pretrained models. We demonstrate the results\nachieved by the benchmarks so far through extensive experiments on\nM2-MedDialog. We release the dataset, the code, as well as the evaluation\nscripts to facilitate future research in this important research direction.", "pdf_link": "https://arxiv.org/pdf/2109.00430"}, "30": {"title": "Title:Impact and dynamics of hate and counter speech online", "authors": "Authors:Joshua Garland, Keyan Ghazi-Zahedi, Jean-Gabriel Young, Laurent H\u00e9bert-Dufresne, Mirta Galesic", "submitted_date": "[Submitted on 16 Sep 2020 (v1), last revised 5 Sep 2021 (this version, v3)]", "abstract": "Abstract:  Citizen-generated counter speech is a promising way to fight hate speech and\npromote peaceful, non-polarized discourse. However, there is a lack of\nlarge-scale longitudinal studies of its effectiveness for reducing hate speech.\nTo this end, we perform an exploratory analysis of the effectiveness of counter\nspeech using several different macro- and micro-level measures to analyze\n180,000 political conversations that took place on German Twitter over four\nyears. We report on the dynamic interactions of hate and counter speech over\ntime and provide insights into whether, as in `classic' bullying situations,\norganized efforts are more effective than independent individuals in steering\nonline discourse. Taken together, our results build a multifaceted picture of\nthe dynamics of hate and counter speech online. While we make no causal claims\ndue to the complexity of discourse dynamics, our findings suggest that\norganized hate speech is associated with changes in public discourse and that\ncounter speech -- especially when organized -- may help curb hateful rhetoric\nin online discourse.", "pdf_link": "https://arxiv.org/pdf/2009.08392"}}