{"0": {"title": "Title:IndicBART: A Pre-trained Model for Natural Language Generation of Indic Languages", "authors": "Authors:Raj Dabre, Himani Shrotriya, Anoop Kunchukuttan, Ratish Puduppully, Mitesh M. Khapra, Pratyush Kumar", "submitted_date": "[Submitted on 7 Sep 2021]", "abstract": "Abstract:  In this paper we present IndicBART, a multilingual, sequence-to-sequence\npre-trained model focusing on 11 Indic languages and English. Different from\nexisting pre-trained models, IndicBART utilizes the orthographic similarity\nbetween Indic scripts to improve transfer learning between similar Indic\nlanguages. We evaluate IndicBART on two NLG tasks: Neural Machine Translation\n(NMT) and extreme summarization. Our experiments on NMT for 12 language pairs\nand extreme summarization for 7 languages using multilingual fine-tuning show\nthat IndicBART is competitive with or better than mBART50 despite containing\nsignificantly fewer parameters. Our analyses focus on identifying the impact of\nscript unification (to Devanagari), corpora size as well as multilingualism on\nthe final performance. The IndicBART model is available under the MIT license\nat this https URL .", "pdf_link": "https://arxiv.org/pdf/2109.02903"}, "1": {"title": "Title:Naturalness Evaluation of Natural Language Generation in Task-oriented Dialogues using BERT", "authors": "Authors:Ye Liu, Wolfgang Maier, Wolfgang Minker, Stefan Ultes", "submitted_date": "[Submitted on 7 Sep 2021]", "abstract": "Abstract:  This paper presents an automatic method to evaluate the naturalness of\nnatural language generation in dialogue systems. While this task was previously\nrendered through expensive and time-consuming human labor, we present this\nnovel task of automatic naturalness evaluation of generated language. By\nfine-tuning the BERT model, our proposed naturalness evaluation method shows\nrobust results and outperforms the baselines: support vector machines,\nbi-directional LSTMs, and BLEURT. In addition, the training speed and\nevaluation performance of naturalness model are improved by transfer learning\nfrom quality and informativeness linguistic knowledge.", "pdf_link": "https://arxiv.org/pdf/2109.02938"}, "2": {"title": "Title:Empathetic Dialogue Generation with Pre-trained RoBERTa-GPT2 and External Knowledge", "authors": "Authors:Ye Liu, Wolfgang Maier, Wolfgang Minker, Stefan Ultes", "submitted_date": "[Submitted on 7 Sep 2021]", "abstract": "Abstract:  One challenge for dialogue agents is to recognize feelings of the\nconversation partner and respond accordingly. In this work, RoBERTa-GPT2 is\nproposed for empathetic dialogue generation, where the pre-trained\nauto-encoding RoBERTa is utilised as encoder and the pre-trained\nauto-regressive GPT-2 as decoder. With the combination of the pre-trained\nRoBERTa and GPT-2, our model realizes a new state-of-the-art emotion accuracy.\nTo enable the empathetic ability of RoBERTa-GPT2 model, we propose a\ncommonsense knowledge and emotional concepts extractor, in which the\ncommonsensible and emotional concepts of dialogue context are extracted for the\nGPT-2 decoder. The experiment results demonstrate that the empathetic dialogue\ngeneration benefits from both pre-trained encoder-decoder architecture and\nexternal knowledge.", "pdf_link": "https://arxiv.org/pdf/2109.03004"}, "3": {"title": "Title:POSSCORE: A Simple Yet Effective Evaluation of Conversational Search with Part of Speech Labelling", "authors": "Authors:Zeyang Liu, Ke Zhou, Jiaxin Mao, Max L. Wilson", "submitted_date": "[Submitted on 7 Sep 2021]", "abstract": "Abstract:  Conversational search systems, such as Google Assistant and Microsoft\nCortana, provide a new search paradigm where users are allowed, via natural\nlanguage dialogues, to communicate with search systems. Evaluating such systems\nis very challenging since search results are presented in the format of natural\nlanguage sentences. Given the unlimited number of possible responses,\ncollecting relevance assessments for all the possible responses is infeasible.\nIn this paper, we propose POSSCORE, a simple yet effective automatic evaluation\nmethod for conversational search. The proposed embedding-based metric takes the\ninfluence of part of speech (POS) of the terms in the response into account. To\nthe best knowledge, our work is the first to systematically demonstrate the\nimportance of incorporating syntactic information, such as POS labels, for\nconversational search evaluation. Experimental results demonstrate that our\nmetrics can correlate with human preference, achieving significant improvements\nover state-of-the-art baseline metrics.", "pdf_link": "https://arxiv.org/pdf/2109.03039"}, "4": {"title": "Title:Learning grounded word meaning representations on similarity graphs", "authors": "Authors:Mariella Dimiccoli, Herwig Wendt, Pau Batlle", "submitted_date": "[Submitted on 7 Sep 2021]", "abstract": "Abstract:  This paper introduces a novel approach to learn visually grounded meaning\nrepresentations of words as low-dimensional node embeddings on an underlying\ngraph hierarchy. The lower level of the hierarchy models modality-specific word\nrepresentations through dedicated but communicating graphs, while the higher\nlevel puts these representations together on a single graph to learn a\nrepresentation jointly from both modalities. The topology of each graph models\nsimilarity relations among words, and is estimated jointly with the graph\nembedding. The assumption underlying this model is that words sharing similar\nmeaning correspond to communities in an underlying similarity graph in a\nlow-dimensional space. We named this model Hierarchical Multi-Modal Similarity\nGraph Embedding (HM-SGE). Experimental results validate the ability of HM-SGE\nto simulate human similarity judgements and concept categorization,\noutperforming the state of the art.", "pdf_link": "https://arxiv.org/pdf/2109.03084"}, "5": {"title": "Title:IndicBART: A Pre-trained Model for Natural Language Generation of Indic Languages", "authors": "Authors:Raj Dabre, Himani Shrotriya, Anoop Kunchukuttan, Ratish Puduppully, Mitesh M. Khapra, Pratyush Kumar", "submitted_date": "[Submitted on 7 Sep 2021]", "abstract": "Abstract:  In this paper we present IndicBART, a multilingual, sequence-to-sequence\npre-trained model focusing on 11 Indic languages and English. Different from\nexisting pre-trained models, IndicBART utilizes the orthographic similarity\nbetween Indic scripts to improve transfer learning between similar Indic\nlanguages. We evaluate IndicBART on two NLG tasks: Neural Machine Translation\n(NMT) and extreme summarization. Our experiments on NMT for 12 language pairs\nand extreme summarization for 7 languages using multilingual fine-tuning show\nthat IndicBART is competitive with or better than mBART50 despite containing\nsignificantly fewer parameters. Our analyses focus on identifying the impact of\nscript unification (to Devanagari), corpora size as well as multilingualism on\nthe final performance. The IndicBART model is available under the MIT license\nat this https URL .", "pdf_link": "https://arxiv.org/pdf/2109.02903"}, "6": {"title": "Title:Naturalness Evaluation of Natural Language Generation in Task-oriented Dialogues using BERT", "authors": "Authors:Ye Liu, Wolfgang Maier, Wolfgang Minker, Stefan Ultes", "submitted_date": "[Submitted on 7 Sep 2021]", "abstract": "Abstract:  This paper presents an automatic method to evaluate the naturalness of\nnatural language generation in dialogue systems. While this task was previously\nrendered through expensive and time-consuming human labor, we present this\nnovel task of automatic naturalness evaluation of generated language. By\nfine-tuning the BERT model, our proposed naturalness evaluation method shows\nrobust results and outperforms the baselines: support vector machines,\nbi-directional LSTMs, and BLEURT. In addition, the training speed and\nevaluation performance of naturalness model are improved by transfer learning\nfrom quality and informativeness linguistic knowledge.", "pdf_link": "https://arxiv.org/pdf/2109.02938"}, "7": {"title": "Title:Empathetic Dialogue Generation with Pre-trained RoBERTa-GPT2 and External Knowledge", "authors": "Authors:Ye Liu, Wolfgang Maier, Wolfgang Minker, Stefan Ultes", "submitted_date": "[Submitted on 7 Sep 2021]", "abstract": "Abstract:  One challenge for dialogue agents is to recognize feelings of the\nconversation partner and respond accordingly. In this work, RoBERTa-GPT2 is\nproposed for empathetic dialogue generation, where the pre-trained\nauto-encoding RoBERTa is utilised as encoder and the pre-trained\nauto-regressive GPT-2 as decoder. With the combination of the pre-trained\nRoBERTa and GPT-2, our model realizes a new state-of-the-art emotion accuracy.\nTo enable the empathetic ability of RoBERTa-GPT2 model, we propose a\ncommonsense knowledge and emotional concepts extractor, in which the\ncommonsensible and emotional concepts of dialogue context are extracted for the\nGPT-2 decoder. The experiment results demonstrate that the empathetic dialogue\ngeneration benefits from both pre-trained encoder-decoder architecture and\nexternal knowledge.", "pdf_link": "https://arxiv.org/pdf/2109.03004"}, "8": {"title": "Title:GOLD: Improving Out-of-Scope Detection in Dialogues using Data Augmentation", "authors": "Authors:Derek Chen, Zhou Yu", "submitted_date": "[Submitted on 7 Sep 2021]", "abstract": "Abstract:  Practical dialogue systems require robust methods of detecting out-of-scope\n(OOS) utterances to avoid conversational breakdowns and related failure modes.\nDirectly training a model with labeled OOS examples yields reasonable\nperformance, but obtaining such data is a resource-intensive process. To tackle\nthis limited-data problem, previous methods focus on better modeling the\ndistribution of in-scope (INS) examples. We introduce GOLD as an orthogonal\ntechnique that augments existing data to train better OOS detectors operating\nin low-data regimes. GOLD generates pseudo-labeled candidates using samples\nfrom an auxiliary dataset and keeps only the most beneficial candidates for\ntraining through a novel filtering mechanism. In experiments across three\ntarget benchmarks, the top GOLD model outperforms all existing methods on all\nkey metrics, achieving relative gains of 52.4%, 48.9% and 50.3% against median\nbaseline performance. We also analyze the unique properties of OOS data to\nidentify key factors for optimally applying our proposed method.", "pdf_link": "https://arxiv.org/pdf/2109.03079"}, "9": {"title": "Title:Learning grounded word meaning representations on similarity graphs", "authors": "Authors:Mariella Dimiccoli, Herwig Wendt, Pau Batlle", "submitted_date": "[Submitted on 7 Sep 2021]", "abstract": "Abstract:  This paper introduces a novel approach to learn visually grounded meaning\nrepresentations of words as low-dimensional node embeddings on an underlying\ngraph hierarchy. The lower level of the hierarchy models modality-specific word\nrepresentations through dedicated but communicating graphs, while the higher\nlevel puts these representations together on a single graph to learn a\nrepresentation jointly from both modalities. The topology of each graph models\nsimilarity relations among words, and is estimated jointly with the graph\nembedding. The assumption underlying this model is that words sharing similar\nmeaning correspond to communities in an underlying similarity graph in a\nlow-dimensional space. We named this model Hierarchical Multi-Modal Similarity\nGraph Embedding (HM-SGE). Experimental results validate the ability of HM-SGE\nto simulate human similarity judgements and concept categorization,\noutperforming the state of the art.", "pdf_link": "https://arxiv.org/pdf/2109.03084"}, "10": {"title": "Title:Unsupervised Conversation Disentanglement through Co-Training", "authors": "Authors:Hui Liu, Zhan Shi, Xiaodan Zhu", "submitted_date": "[Submitted on 7 Sep 2021]", "abstract": "Abstract:  Conversation disentanglement aims to separate intermingled messages into\ndetached sessions, which is a fundamental task in understanding multi-party\nconversations. Existing work on conversation disentanglement relies heavily\nupon human-annotated datasets, which are expensive to obtain in practice. In\nthis work, we explore to train a conversation disentanglement model without\nreferencing any human annotations. Our method is built upon a deep co-training\nalgorithm, which consists of two neural networks: a message-pair classifier and\na session classifier. The former is responsible for retrieving local relations\nbetween two messages while the latter categorizes a message to a session by\ncapturing context-aware information. Both networks are initialized respectively\nwith pseudo data built from an unannotated corpus. During the deep co-training\nprocess, we use the session classifier as a reinforcement learning component to\nlearn a session assigning policy by maximizing the local rewards given by the\nmessage-pair classifier. For the message-pair classifier, we enrich its\ntraining data by retrieving message pairs with high confidence from the\ndisentangled sessions predicted by the session classifier. Experimental results\non the large Movie Dialogue Dataset demonstrate that our proposed approach\nachieves competitive performance compared to the previous supervised methods.\nFurther experiments show that the predicted disentangled conversations can\npromote the performance on the downstream task of multi-party response\nselection.", "pdf_link": "https://arxiv.org/pdf/2109.03199"}, "11": {"title": "Title:Joint model for intent and entity recognition", "authors": "Authors:Petr Lorenc", "submitted_date": "[Submitted on 7 Sep 2021]", "abstract": "Abstract:  The semantic understanding of natural dialogues composes of several parts.\nSome of them, like intent classification and entity detection, have a crucial\nrole in deciding the next steps in handling user input. Handling each task as\nan individual problem can be wasting of training resources, and also each\nproblem can benefit from each other. This paper tackles these problems as one.\nOur new model, which combine intent and entity recognition into one system, is\nachieving better metrics in both tasks with lower training requirements than\nsolving each task separately. We also optimize the model based on the inputs.", "pdf_link": "https://arxiv.org/pdf/2109.03221"}, "12": {"title": "Title:WhyAct: Identifying Action Reasons in Lifestyle Vlogs", "authors": "Authors:Oana Ignat, Santiago Castro, Hanwen Miao, Weiji Li, Rada Mihalcea", "submitted_date": "[Submitted on 6 Sep 2021]", "abstract": "Abstract:  We aim to automatically identify human action reasons in online videos. We\nfocus on the widespread genre of lifestyle vlogs, in which people perform\nactions while verbally describing them. We introduce and make publicly\navailable the {\\sc WhyAct} dataset, consisting of 1,077 visual actions manually\nannotated with their reasons. We describe a multimodal model that leverages\nvisual and textual information to automatically infer the reasons corresponding\nto an action presented in the video.", "pdf_link": "https://arxiv.org/pdf/2109.02747"}, "13": {"title": "Title:POSSCORE: A Simple Yet Effective Evaluation of Conversational Search with Part of Speech Labelling", "authors": "Authors:Zeyang Liu, Ke Zhou, Jiaxin Mao, Max L. Wilson", "submitted_date": "[Submitted on 7 Sep 2021]", "abstract": "Abstract:  Conversational search systems, such as Google Assistant and Microsoft\nCortana, provide a new search paradigm where users are allowed, via natural\nlanguage dialogues, to communicate with search systems. Evaluating such systems\nis very challenging since search results are presented in the format of natural\nlanguage sentences. Given the unlimited number of possible responses,\ncollecting relevance assessments for all the possible responses is infeasible.\nIn this paper, we propose POSSCORE, a simple yet effective automatic evaluation\nmethod for conversational search. The proposed embedding-based metric takes the\ninfluence of part of speech (POS) of the terms in the response into account. To\nthe best knowledge, our work is the first to systematically demonstrate the\nimportance of incorporating syntactic information, such as POS labels, for\nconversational search evaluation. Experimental results demonstrate that our\nmetrics can correlate with human preference, achieving significant improvements\nover state-of-the-art baseline metrics.", "pdf_link": "https://arxiv.org/pdf/2109.03039"}, "14": {"title": "Title:Query-Variant Advertisement Text Generation with Association Knowledge", "authors": "Authors:Siyu Duan, Wei Li, Cai Jing, Yancheng He, Yunfang Wu, Xu Sun", "submitted_date": "[Submitted on 14 Apr 2020 (v1), last revised 7 Sep 2021 (this version, v2)]", "abstract": "Abstract:  Advertising is an important revenue source for many companies. However, it is\nexpensive to manually create advertisements that meet the needs of various\nqueries for massive items. In this paper, we propose the query-variant\nadvertisement text generation task that aims to generate candidate\nadvertisements for different queries with various needs given the item\nkeywords. In this task, for many different queries there is only one general\npurposed advertisement with no predefined query-advertisement pair, which would\ndiscourage traditional End-to-End models from generating query-variant\nadvertisements for different queries with different needs. To deal with the\nproblem, we propose a query-variant advertisement text generation model that\ntakes keywords and associated external knowledge as input during training and\nadds different queries during inference. Adding external knowledge helps the\nmodel adapted to the information besides the item keywords during training,\nwhich makes the transition between training and inference more smoothing when\nthe query is added during inference. Both automatic and human evaluation show\nthat our model can generate more attractive and query-focused advertisements\nthan the strong baselines.", "pdf_link": "https://arxiv.org/pdf/2004.06438"}, "15": {"title": "Title:Injecting Entity Types into Entity-Guided Text Generation", "authors": "Authors:Xiangyu Dong, Wenhao Yu, Chenguang Zhu, Meng Jiang", "submitted_date": "[Submitted on 28 Sep 2020 (v1), last revised 7 Sep 2021 (this version, v3)]", "abstract": "Abstract:  Recent successes in deep generative modeling have led to significant advances\nin natural language generation (NLG). Incorporating entities into neural\ngeneration models has demonstrated great improvements by assisting to infer the\nsummary topic and to generate coherent content. To enhance the role of entity\nin NLG, in this paper, we aim to model the entity type in the decoding phase to\ngenerate contextual words accurately. We develop a novel NLG model to produce a\ntarget sequence based on a given list of entities. Our model has a multi-step\ndecoder that injects the entity types into the process of entity mention\ngeneration. Experiments on two public news datasets demonstrate type injection\nperforms better than existing type embedding concatenation baselines.", "pdf_link": "https://arxiv.org/pdf/2009.13401"}, "16": {"title": "Title:Data-QuestEval: A Referenceless Metric for Data-to-Text Semantic Evaluation", "authors": "Authors:Cl\u00e9ment Rebuffel, Thomas Scialom, Laure Soulier, Benjamin Piwowarski, Sylvain Lamprier, Jacopo Staiano, Geoffrey Scoutheeten, Patrick Gallinari", "submitted_date": "[Submitted on 15 Apr 2021 (v1), last revised 7 Sep 2021 (this version, v3)]", "abstract": "Abstract:  QuestEval is a reference-less metric used in text-to-text tasks, that\ncompares the generated summaries directly to the source text, by automatically\nasking and answering questions. Its adaptation to Data-to-Text tasks is not\nstraightforward, as it requires multimodal Question Generation and Answering\nsystems on the considered tasks, which are seldom available. To this purpose,\nwe propose a method to build synthetic multimodal corpora enabling to train\nmultimodal components for a data-QuestEval metric. The resulting metric is\nreference-less and multimodal; it obtains state-of-the-art correlations with\nhuman judgment on the WebNLG and WikiBio benchmarks. We make data-QuestEval's\ncode and models available for reproducibility purpose, as part of the QuestEval\nproject.", "pdf_link": "https://arxiv.org/pdf/2104.07555"}}