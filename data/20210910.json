{"0": {"title": "Title:Transformers in Vision: A Survey", "authors": "Authors:Salman Khan, Muzammal Naseer, Munawar Hayat, Syed Waqas Zamir, Fahad Shahbaz Khan, Mubarak Shah", "submitted_date": "[Submitted on 4 Jan 2021 (v1), last revised 8 Sep 2021 (this version, v3)]", "abstract": "Abstract:  Astounding results from Transformer models on natural language tasks have\nintrigued the vision community to study their application to computer vision\nproblems. Among their salient benefits, Transformers enable modeling long\ndependencies between input sequence elements and support parallel processing of\nsequence as compared to recurrent networks e.g., Long short-term memory (LSTM).\nDifferent from convolutional networks, Transformers require minimal inductive\nbiases for their design and are naturally suited as set-functions. Furthermore,\nthe straightforward design of Transformers allows processing multiple\nmodalities (e.g., images, videos, text and speech) using similar processing\nblocks and demonstrates excellent scalability to very large capacity networks\nand huge datasets. These strengths have led to exciting progress on a number of\nvision tasks using Transformer networks. This survey aims to provide a\ncomprehensive overview of the Transformer models in the computer vision\ndiscipline. We start with an introduction to fundamental concepts behind the\nsuccess of Transformers i.e., self-attention, large-scale pre-training, and\nbidirectional encoding. We then cover extensive applications of transformers in\nvision including popular recognition tasks (e.g., image classification, object\ndetection, action recognition, and segmentation), generative modeling,\nmulti-modal tasks (e.g., visual-question answering, visual reasoning, and\nvisual grounding), video processing (e.g., activity recognition, video\nforecasting), low-level vision (e.g., image super-resolution, image\nenhancement, and colorization) and 3D analysis (e.g., point cloud\nclassification and segmentation). We compare the respective advantages and\nlimitations of popular techniques both in terms of architectural design and\ntheir experimental value. Finally, we provide an analysis on open research\ndirections and possible future works.", "pdf_link": "https://arxiv.org/pdf/2101.01169"}, "1": {"title": "Title:Effective Sequence-to-Sequence Dialogue State Tracking", "authors": "Authors:Jeffrey Zhao, Mahdis Mahdieh, Ye Zhang, Yuan Cao, Yonghui Wu", "submitted_date": "[Submitted on 31 Aug 2021 (v1), last revised 8 Sep 2021 (this version, v2)]", "abstract": "Abstract:  Sequence-to-sequence models have been applied to a wide variety of NLP tasks,\nbut how to properly use them for dialogue state tracking has not been\nsystematically investigated. In this paper, we study this problem from the\nperspectives of pre-training objectives as well as the formats of context\nrepresentations. We demonstrate that the choice of pre-training objective makes\na significant difference to the state tracking quality. In particular, we find\nthat masked span prediction is more effective than auto-regressive language\nmodeling. We also explore using Pegasus, a span prediction-based pre-training\nobjective for text summarization, for the state tracking model. We found that\npre-training for the seemingly distant summarization task works surprisingly\nwell for dialogue state tracking. In addition, we found that while recurrent\nstate context representation works also reasonably well, the model may have a\nhard time recovering from earlier mistakes. We conducted experiments on the\nMultiWOZ 2.1-2.4, WOZ 2.0, and DSTC2 datasets with consistent observations.", "pdf_link": "https://arxiv.org/pdf/2108.13990"}, "2": {"title": "Title:M^2-MedDialog: A Dataset and Benchmarks for Multi-domain Multi-service Medical Dialogues", "authors": "Authors:Guojun Yan, Jiahuan Pei, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Huasheng Liang", "submitted_date": "[Submitted on 1 Sep 2021 (v1), last revised 8 Sep 2021 (this version, v3)]", "abstract": "Abstract:  Medical dialogue systems (MDSs) aim to assist doctors and patients with a\nrange of professional medical services, i.e., diagnosis, consultation, and\ntreatment. However, one-stop MDS is still unexplored because: (1) no dataset\nhas so large-scale dialogues contains both multiple medical services and\nfine-grained medical labels (i.e., intents, slots, values); (2) no model has\naddressed a MDS based on multiple-service conversations in a unified framework.\nIn this work, we first build a Multiple-domain Multiple-service medical\ndialogue (M^2-MedDialog)dataset, which contains 1,557 conversations between\ndoctors and patients, covering 276 types of diseases, 2,468 medical entities,\nand 3 specialties of medical services. To the best of our knowledge, it is the\nonly medical dialogue dataset that includes both multiple medical services and\nfine-grained medical labels. Then, we formulate a one-stop MDS as a\nsequence-to-sequence generation problem. We unify a MDS with causal language\nmodeling and conditional causal language modeling, respectively. Specifically,\nwe employ several pretrained models (i.e., BERT-WWM, BERT-MED, GPT2, and MT5)\nand their variants to get benchmarks on M^2-MedDialog dataset. We also propose\npseudo labeling and natural perturbation methods to expand M2-MedDialog dataset\nand enhance the state-of-the-art pretrained models. We demonstrate the results\nachieved by the benchmarks so far through extensive experiments on\nM2-MedDialog. We release the dataset, the code, as well as the evaluation\nscripts to facilitate future research in this important research direction.", "pdf_link": "https://arxiv.org/pdf/2109.00430"}, "3": {"title": "Title:Hi, my name is Martha: Using names to measure and mitigate bias in generative dialogue models", "authors": "Authors:Eric Michael Smith, Adina Williams", "submitted_date": "[Submitted on 7 Sep 2021]", "abstract": "Abstract:  All AI models are susceptible to learning biases in data that they are\ntrained on. For generative dialogue models, being trained on real human\nconversations containing unbalanced gender and race/ethnicity references can\nlead to models that display learned biases, which we define here broadly as any\nmeasurable differences in the distributions of words or semantic content of\nconversations based on demographic groups. We measure the strength of such\nbiases by producing artificial conversations between two copies of a dialogue\nmodel, conditioning one conversational partner to state a name commonly\nassociated with a certain gender and/or race/ethnicity. We find that larger\ncapacity models tend to exhibit more gender bias and greater stereotyping of\noccupations by gender. We show that several methods of tuning these dialogue\nmodels, specifically name scrambling, controlled generation, and unlikelihood\ntraining, are effective in reducing bias in conversation, including on a\ndownstream conversational task. Name scrambling is also effective in lowering\ndifferences in token usage across conversations where partners have names\nassociated with different genders or races/ethnicities.", "pdf_link": "https://arxiv.org/pdf/2109.03300"}, "4": {"title": "Title:Vision Matters When It Should: Sanity Checking Multimodal Machine Translation Models", "authors": "Authors:Jiaoda Li, Duygu Ataman, Rico Sennrich", "submitted_date": "[Submitted on 8 Sep 2021]", "abstract": "Abstract:  Multimodal machine translation (MMT) systems have been shown to outperform\ntheir text-only neural machine translation (NMT) counterparts when visual\ncontext is available. However, recent studies have also shown that the\nperformance of MMT models is only marginally impacted when the associated image\nis replaced with an unrelated image or noise, which suggests that the visual\ncontext might not be exploited by the model at all. We hypothesize that this\nmight be caused by the nature of the commonly used evaluation benchmark, also\nknown as Multi30K, where the translations of image captions were prepared\nwithout actually showing the images to human translators. In this paper, we\npresent a qualitative study that examines the role of datasets in stimulating\nthe leverage of visual modality and we propose methods to highlight the\nimportance of visual signals in the datasets which demonstrate improvements in\nreliance of models on the source images. Our findings suggest the research on\neffective MMT architectures is currently impaired by the lack of suitable\ndatasets and careful consideration must be taken in creation of future MMT\ndatasets, for which we also provide useful insights.", "pdf_link": "https://arxiv.org/pdf/2109.03415"}, "5": {"title": "Title:Sequence Level Contrastive Learning for Text Summarization", "authors": "Authors:Shusheng Xu, Xingxing Zhang, Yi Wu, Furu Wei", "submitted_date": "[Submitted on 8 Sep 2021]", "abstract": "Abstract:  Contrastive learning models have achieved great success in unsupervised\nvisual representation learning, which maximize the similarities between feature\nrepresentations of different views of the same image, while minimize the\nsimilarities between feature representations of views of different images. In\ntext summarization, the output summary is a shorter form of the input document\nand they have similar meanings. In this paper, we propose a contrastive\nlearning model for supervised abstractive text summarization, where we view a\ndocument, its gold summary and its model generated summaries as different views\nof the same mean representation and maximize the similarities between them\nduring training. We improve over a strong sequence-to-sequence text generation\nmodel (i.e., BART) on three different summarization datasets. Human evaluation\nalso shows that our model achieves better faithfulness ratings compared to its\ncounterpart without contrastive objectives.", "pdf_link": "https://arxiv.org/pdf/2109.03481"}, "6": {"title": "Title:Self- and Pseudo-self-supervised Prediction of Speaker and Key-utterance for Multi-party Dialogue Reading Comprehension", "authors": "Authors:Yiyang Li, Hai Zhao", "submitted_date": "[Submitted on 8 Sep 2021]", "abstract": "Abstract:  Multi-party dialogue machine reading comprehension (MRC) brings tremendous\nchallenge since it involves multiple speakers at one dialogue, resulting in\nintricate speaker information flows and noisy dialogue contexts. To alleviate\nsuch difficulties, previous models focus on how to incorporate these\ninformation using complex graph-based modules and additional manually labeled\ndata, which is usually rare in real scenarios. In this paper, we design two\nlabour-free self- and pseudo-self-supervised prediction tasks on speaker and\nkey-utterance to implicitly model the speaker information flows, and capture\nsalient clues in a long dialogue. Experimental results on two benchmark\ndatasets have justified the effectiveness of our method over competitive\nbaselines and current state-of-the-art models.", "pdf_link": "https://arxiv.org/pdf/2109.03772"}, "7": {"title": "Title:Smelting Gold and Silver for Improved Multilingual AMR-to-Text Generation", "authors": "Authors:Leonardo F. R. Ribeiro, Jonas Pfeiffer, Yue Zhang, Iryna Gurevych", "submitted_date": "[Submitted on 8 Sep 2021]", "abstract": "Abstract:  Recent work on multilingual AMR-to-text generation has exclusively focused on\ndata augmentation strategies that utilize silver AMR. However, this assumes a\nhigh quality of generated AMRs, potentially limiting the transferability to the\ntarget task. In this paper, we investigate different techniques for\nautomatically generating AMR annotations, where we aim to study which source of\ninformation yields better multilingual results. Our models trained on gold AMR\nwith silver (machine translated) sentences outperform approaches which leverage\ngenerated silver AMR. We find that combining both complementary sources of\ninformation further improves multilingual AMR-to-text generation. Our models\nsurpass the previous state of the art for German, Italian, Spanish, and Chinese\nby a large margin.", "pdf_link": "https://arxiv.org/pdf/2109.03808"}, "8": {"title": "Title:TrollsWithOpinion: A Dataset for Predicting Domain-specific Opinion Manipulation in Troll Memes", "authors": "Authors:Shardul Suryawanshi, Bharathi Raja Chakravarthi, Mihael Arcan, Suzanne Little, Paul Buitelaar", "submitted_date": "[Submitted on 8 Sep 2021]", "abstract": "Abstract:  Research into the classification of Image with Text (IWT) troll memes has\nrecently become popular. Since the online community utilizes the refuge of\nmemes to express themselves, there is an abundance of data in the form of\nmemes. These memes have the potential to demean, harras, or bully targeted\nindividuals. Moreover, the targeted individual could fall prey to opinion\nmanipulation. To comprehend the use of memes in opinion manipulation, we define\nthree specific domains (product, political or others) which we classify into\ntroll or not-troll, with or without opinion manipulation. To enable this\nanalysis, we enhanced an existing dataset by annotating the data with our\ndefined classes, resulting in a dataset of 8,881 IWT or multimodal memes in the\nEnglish language (TrollsWithOpinion dataset). We perform baseline experiments\non the annotated dataset, and our result shows that existing state-of-the-art\ntechniques could only reach a weighted-average F1-score of 0.37. This shows the\nneed for a development of a specific technique to deal with multimodal troll\nmemes.", "pdf_link": "https://arxiv.org/pdf/2109.03571"}, "9": {"title": "Title:Structural Adapters in Pretrained Language Models for AMR-to-text Generation", "authors": "Authors:Leonardo F. R. Ribeiro, Yue Zhang, Iryna Gurevych", "submitted_date": "[Submitted on 16 Mar 2021 (v1), last revised 8 Sep 2021 (this version, v2)]", "abstract": "Abstract:  Pretrained language models (PLM) have recently advanced graph-to-text\ngeneration, where the input graph is linearized into a sequence and fed into\nthe PLM to obtain its representation. However, efficiently encoding the graph\nstructure in PLMs is challenging because such models were pretrained on natural\nlanguage, and modeling structured data may lead to catastrophic forgetting of\ndistributional knowledge. In this paper, we propose StructAdapt, an adapter\nmethod to encode graph structure into PLMs. Contrary to prior work, StructAdapt\neffectively models interactions among the nodes based on the graph\nconnectivity, only training graph structure-aware adapter parameters. In this\nway, we incorporate task-specific knowledge while maintaining the topological\nstructure of the graph. We empirically show the benefits of explicitly encoding\ngraph structure into PLMs using StructAdapt, outperforming the state of the art\non two AMR-to-text datasets, training only 5.1% of the PLM parameters.", "pdf_link": "https://arxiv.org/pdf/2103.09120"}, "10": {"title": "Title:Effective Sequence-to-Sequence Dialogue State Tracking", "authors": "Authors:Jeffrey Zhao, Mahdis Mahdieh, Ye Zhang, Yuan Cao, Yonghui Wu", "submitted_date": "[Submitted on 31 Aug 2021 (v1), last revised 8 Sep 2021 (this version, v2)]", "abstract": "Abstract:  Sequence-to-sequence models have been applied to a wide variety of NLP tasks,\nbut how to properly use them for dialogue state tracking has not been\nsystematically investigated. In this paper, we study this problem from the\nperspectives of pre-training objectives as well as the formats of context\nrepresentations. We demonstrate that the choice of pre-training objective makes\na significant difference to the state tracking quality. In particular, we find\nthat masked span prediction is more effective than auto-regressive language\nmodeling. We also explore using Pegasus, a span prediction-based pre-training\nobjective for text summarization, for the state tracking model. We found that\npre-training for the seemingly distant summarization task works surprisingly\nwell for dialogue state tracking. In addition, we found that while recurrent\nstate context representation works also reasonably well, the model may have a\nhard time recovering from earlier mistakes. We conducted experiments on the\nMultiWOZ 2.1-2.4, WOZ 2.0, and DSTC2 datasets with consistent observations.", "pdf_link": "https://arxiv.org/pdf/2108.13990"}, "11": {"title": "Title:M^2-MedDialog: A Dataset and Benchmarks for Multi-domain Multi-service Medical Dialogues", "authors": "Authors:Guojun Yan, Jiahuan Pei, Pengjie Ren, Zhumin Chen, Zhaochun Ren, Huasheng Liang", "submitted_date": "[Submitted on 1 Sep 2021 (v1), last revised 8 Sep 2021 (this version, v3)]", "abstract": "Abstract:  Medical dialogue systems (MDSs) aim to assist doctors and patients with a\nrange of professional medical services, i.e., diagnosis, consultation, and\ntreatment. However, one-stop MDS is still unexplored because: (1) no dataset\nhas so large-scale dialogues contains both multiple medical services and\nfine-grained medical labels (i.e., intents, slots, values); (2) no model has\naddressed a MDS based on multiple-service conversations in a unified framework.\nIn this work, we first build a Multiple-domain Multiple-service medical\ndialogue (M^2-MedDialog)dataset, which contains 1,557 conversations between\ndoctors and patients, covering 276 types of diseases, 2,468 medical entities,\nand 3 specialties of medical services. To the best of our knowledge, it is the\nonly medical dialogue dataset that includes both multiple medical services and\nfine-grained medical labels. Then, we formulate a one-stop MDS as a\nsequence-to-sequence generation problem. We unify a MDS with causal language\nmodeling and conditional causal language modeling, respectively. Specifically,\nwe employ several pretrained models (i.e., BERT-WWM, BERT-MED, GPT2, and MT5)\nand their variants to get benchmarks on M^2-MedDialog dataset. We also propose\npseudo labeling and natural perturbation methods to expand M2-MedDialog dataset\nand enhance the state-of-the-art pretrained models. We demonstrate the results\nachieved by the benchmarks so far through extensive experiments on\nM2-MedDialog. We release the dataset, the code, as well as the evaluation\nscripts to facilitate future research in this important research direction.", "pdf_link": "https://arxiv.org/pdf/2109.00430"}, "12": {"title": "Title:Vision Guided Generative Pre-trained Language Models for Multimodal Abstractive Summarization", "authors": "Authors:Tiezheng Yu, Wenliang Dai, Zihan Liu, Pascale Fung", "submitted_date": "[Submitted on 6 Sep 2021 (v1), last revised 8 Sep 2021 (this version, v2)]", "abstract": "Abstract:  Multimodal abstractive summarization (MAS) models that summarize videos\n(vision modality) and their corresponding transcripts (text modality) are able\nto extract the essential information from massive multimodal data on the\nInternet. Recently, large-scale generative pre-trained language models (GPLMs)\nhave been shown to be effective in text generation tasks. However, existing MAS\nmodels cannot leverage GPLMs' powerful generation ability. To fill this\nresearch gap, we aim to study two research questions: 1) how to inject visual\ninformation into GPLMs without hurting their generation ability; and 2) where\nis the optimal place in GPLMs to inject the visual information? In this paper,\nwe present a simple yet effective method to construct vision guided (VG) GPLMs\nfor the MAS task using attention-based add-on layers to incorporate visual\ninformation while maintaining their original text generation ability. Results\nshow that our best model significantly surpasses the prior state-of-the-art\nmodel by 5.7 ROUGE-1, 5.3 ROUGE-2, and 5.1 ROUGE-L scores on the How2 dataset,\nand our visual guidance method contributes 83.6% of the overall improvement.\nFurthermore, we conduct thorough ablation studies to analyze the effectiveness\nof various modality fusion methods and fusion locations.", "pdf_link": "https://arxiv.org/pdf/2109.02401"}}