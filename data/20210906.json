{"0": {"title": "Title:CX-ToM: Counterfactual Explanations with Theory-of-Mind for Enhancing Human Trust in Image Recognition Models", "authors": "Authors:Arjun R. Akula, Keze Wang, Changsong Liu, Sari Saba-Sadiya, Hongjing Lu, Sinisa Todorovic, Joyce Chai, Song-Chun Zhu", "submitted_date": "[Submitted on 3 Sep 2021]", "abstract": "Abstract:  We propose CX-ToM, short for counterfactual explanations with theory-of mind,\na new explainable AI (XAI) framework for explaining decisions made by a deep\nconvolutional neural network (CNN). In contrast to the current methods in XAI\nthat generate explanations as a single shot response, we pose explanation as an\niterative communication process, i.e. dialog, between the machine and human\nuser. More concretely, our CX-ToM framework generates sequence of explanations\nin a dialog by mediating the differences between the minds of machine and human\nuser. To do this, we use Theory of Mind (ToM) which helps us in explicitly\nmodeling human's intention, machine's mind as inferred by the human as well as\nhuman's mind as inferred by the machine. Moreover, most state-of-the-art XAI\nframeworks provide attention (or heat map) based explanations. In our work, we\nshow that these attention based explanations are not sufficient for increasing\nhuman trust in the underlying CNN model. In CX-ToM, we instead use\ncounterfactual explanations called fault-lines which we define as follows:\ngiven an input image I for which a CNN classification model M predicts class\nc_pred, a fault-line identifies the minimal semantic-level features (e.g.,\nstripes on zebra, pointed ears of dog), referred to as explainable concepts,\nthat need to be added to or deleted from I in order to alter the classification\ncategory of I by M to another specified class c_alt. We argue that, due to the\niterative, conceptual and counterfactual nature of CX-ToM explanations, our\nframework is practical and more natural for both expert and non-expert users to\nunderstand the internal workings of complex deep learning models. Extensive\nquantitative and qualitative experiments verify our hypotheses, demonstrating\nthat our CX-ToM significantly outperforms the state-of-the-art explainable AI\nmodels.", "pdf_link": "https://arxiv.org/pdf/2109.01401"}, "1": {"title": "Title:A Longitudinal Multi-modal Dataset for Dementia Monitoring and Diagnosis", "authors": "Authors:Dimitris Gkoumas, Bo Wang, Adam Tsakalidis, Maria Wolters, Arkaitz Zubiaga, Matthew Purver, Maria Liakata", "submitted_date": "[Submitted on 3 Sep 2021]", "abstract": "Abstract:  Dementia is a family of neurogenerative conditions affecting memory and\ncognition in an increasing number of individuals in our globally aging\npopulation. Automated analysis of language, speech and paralinguistic\nindicators have been gaining popularity as potential indicators of cognitive\ndecline. Here we propose a novel longitudinal multi-modal dataset collected\nfrom people with mild dementia and age matched controls over a period of\nseveral months in a natural setting. The multi-modal data consists of spoken\nconversations, a subset of which are transcribed, as well as typed and written\nthoughts and associated extra-linguistic information such as pen strokes and\nkeystrokes. We describe the dataset in detail and proceed to focus on a task\nusing the speech modality. The latter involves distinguishing controls from\npeople with dementia by exploiting the longitudinal nature of the data. Our\nexperiments showed significant differences in how the speech varied from\nsession to session in the control and dementia groups.", "pdf_link": "https://arxiv.org/pdf/2109.01537"}, "2": {"title": "Title:Learning Abstract Representations through Lossy Compression of Multi-Modal Signals", "authors": "Authors:Charles Wilmot, Gianluca Baldassarre, Jochen Triesch", "submitted_date": "[Submitted on 27 Jan 2021 (v1), last revised 3 Sep 2021 (this version, v3)]", "abstract": "Abstract:  A key competence for open-ended learning is the formation of increasingly\nabstract representations useful for driving complex behavior. Abstract\nrepresentations ignore specific details and facilitate generalization. Here we\nconsider the learning of abstract representations in a multi-modal setting with\ntwo or more input modalities. We treat the problem as a lossy compression\nproblem and show that generic lossy compression of multimodal sensory input\nnaturally extracts abstract representations that tend to strip away modalitiy\nspecific details and preferentially retain information that is shared across\nthe different modalities. Furthermore, we propose an architecture to learn\nabstract representations by identifying and retaining only the information that\nis shared across multiple modalities while discarding any modality specific\ninformation.", "pdf_link": "https://arxiv.org/pdf/2101.11376"}, "3": {"title": "Title:Multimodal Conditionality for Natural Language Generation", "authors": "Authors:Michael Sollami, Aashish Jain", "submitted_date": "[Submitted on 2 Sep 2021]", "abstract": "Abstract:  Large scale pretrained language models have demonstrated state-of-the-art\nperformance in language understanding tasks. Their application has recently\nexpanded into multimodality learning, leading to improved representations\ncombining vision and language. However, progress in adapting language models\ntowards conditional Natural Language Generation (NLG) has been limited to a\nsingle modality, generally text. We propose MAnTiS, Multimodal Adaptation for\nText Synthesis, a general approach for multimodal conditionality in\ntransformer-based NLG models. In this method, we pass inputs from each modality\nthrough modality-specific encoders, project to textual token space, and finally\njoin to form a conditionality prefix. We fine-tune the pretrained language\nmodel and encoders with the conditionality prefix guiding the generation. We\napply MAnTiS to the task of product description generation, conditioning a\nnetwork on both product images and titles to generate descriptive text. We\ndemonstrate that MAnTiS outperforms strong baseline approaches on standard NLG\nscoring metrics. Furthermore, qualitative assessments demonstrate that MAnTiS\ncan generate human quality descriptions consistent with given multimodal\ninputs.", "pdf_link": "https://arxiv.org/pdf/2109.01229"}, "4": {"title": "Title:A Context-Aware Hierarchical BERT Fusion Network for Multi-turn Dialog Act Detection", "authors": "Authors:Ting-Wei Wu, Ruolin Su, Biing-Hwang Juang", "submitted_date": "[Submitted on 3 Sep 2021]", "abstract": "Abstract:  The success of interactive dialog systems is usually associated with the\nquality of the spoken language understanding (SLU) task, which mainly\nidentifies the corresponding dialog acts and slot values in each turn. By\ntreating utterances in isolation, most SLU systems often overlook the semantic\ncontext in which a dialog act is expected. The act dependency between turns is\nnon-trivial and yet critical to the identification of the correct semantic\nrepresentations. Previous works with limited context awareness have exposed the\ninadequacy of dealing with complexity in multiproned user intents, which are\nsubject to spontaneous change during turn transitions. In this work, we propose\nto enhance SLU in multi-turn dialogs, employing a context-aware hierarchical\nBERT fusion Network (CaBERT-SLU) to not only discern context information within\na dialog but also jointly identify multiple dialog acts and slots in each\nutterance. Experimental results show that our approach reaches new\nstate-of-the-art (SOTA) performances in two complicated multi-turn dialogue\ndatasets with considerable improvements compared with previous methods, which\nonly consider single utterances for multiple intents and slot filling.", "pdf_link": "https://arxiv.org/pdf/2109.01267"}, "5": {"title": "Title:A Longitudinal Multi-modal Dataset for Dementia Monitoring and Diagnosis", "authors": "Authors:Dimitris Gkoumas, Bo Wang, Adam Tsakalidis, Maria Wolters, Arkaitz Zubiaga, Matthew Purver, Maria Liakata", "submitted_date": "[Submitted on 3 Sep 2021]", "abstract": "Abstract:  Dementia is a family of neurogenerative conditions affecting memory and\ncognition in an increasing number of individuals in our globally aging\npopulation. Automated analysis of language, speech and paralinguistic\nindicators have been gaining popularity as potential indicators of cognitive\ndecline. Here we propose a novel longitudinal multi-modal dataset collected\nfrom people with mild dementia and age matched controls over a period of\nseveral months in a natural setting. The multi-modal data consists of spoken\nconversations, a subset of which are transcribed, as well as typed and written\nthoughts and associated extra-linguistic information such as pen strokes and\nkeystrokes. We describe the dataset in detail and proceed to focus on a task\nusing the speech modality. The latter involves distinguishing controls from\npeople with dementia by exploiting the longitudinal nature of the data. Our\nexperiments showed significant differences in how the speech varied from\nsession to session in the control and dementia groups.", "pdf_link": "https://arxiv.org/pdf/2109.01537"}, "6": {"title": "Title:Biomedical Data-to-Text Generation via Fine-Tuning Transformers", "authors": "Authors:Ruslan Yermakov, Nicholas Drago, Angelo Ziletti", "submitted_date": "[Submitted on 3 Sep 2021]", "abstract": "Abstract:  Data-to-text (D2T) generation in the biomedical domain is a promising - yet\nmostly unexplored - field of research. Here, we apply neural models for D2T\ngeneration to a real-world dataset consisting of package leaflets of European\nmedicines. We show that fine-tuned transformers are able to generate realistic,\nmultisentence text from data in the biomedical domain, yet have important\nlimitations. We also release a new dataset (BioLeaflets) for benchmarking D2T\ngeneration models in the biomedical domain.", "pdf_link": "https://arxiv.org/pdf/2109.01518"}, "7": {"title": "Title:CR-Walker: Tree-Structured Graph Reasoning and Dialog Acts for Conversational Recommendation", "authors": "Authors:Wenchang Ma, Ryuichi Takanobu, Minlie Huang", "submitted_date": "[Submitted on 20 Oct 2020 (v1), last revised 3 Sep 2021 (this version, v2)]", "abstract": "Abstract:  Growing interests have been attracted in Conversational Recommender Systems\n(CRS), which explore user preference through conversational interactions in\norder to make appropriate recommendation. However, there is still a lack of\nability in existing CRS to (1) traverse multiple reasoning paths over\nbackground knowledge to introduce relevant items and attributes, and (2)\narrange selected entities appropriately under current system intents to control\nresponse generation. To address these issues, we propose CR-Walker in this\npaper, a model that performs tree-structured reasoning on a knowledge graph,\nand generates informative dialog acts to guide language generation. The unique\nscheme of tree-structured reasoning views the traversed entity at each hop as\npart of dialog acts to facilitate language generation, which links how entities\nare selected and expressed. Automatic and human evaluations show that CR-Walker\ncan arrive at more accurate recommendation, and generate more informative and\nengaging responses.", "pdf_link": "https://arxiv.org/pdf/2010.10333"}, "8": {"title": "Title:Extract, Denoise and Enforce: Evaluating and Improving Concept Preservation for Text-to-Text Generation", "authors": "Authors:Yuning Mao, Wenchang Ma, Deren Lei, Jiawei Han, Xiang Ren", "submitted_date": "[Submitted on 18 Apr 2021 (v1), last revised 3 Sep 2021 (this version, v2)]", "abstract": "Abstract:  Prior studies on text-to-text generation typically assume that the model\ncould figure out what to attend to in the input and what to include in the\noutput via seq2seq learning, with only the parallel training data and no\nadditional guidance. However, it remains unclear whether current models can\npreserve important concepts in the source input, as seq2seq learning does not\nhave explicit focus on the concepts and commonly used evaluation metrics also\ntreat concepts equally important as other tokens. In this paper, we present a\nsystematic analysis that studies whether current seq2seq models, especially\npre-trained language models, are good enough for preserving important input\nconcepts and to what extent explicitly guiding generation with the concepts as\nlexical constraints is beneficial. We answer the above questions by conducting\nextensive analytical experiments on four representative text-to-text generation\ntasks. Based on the observations, we then propose a simple yet effective\nframework to automatically extract, denoise, and enforce important input\nconcepts as lexical constraints. This new method performs comparably or better\nthan its unconstrained counterpart on automatic metrics, demonstrates higher\ncoverage for concept preservation, and receives better ratings in the human\nevaluation. Our code is available at this https URL.", "pdf_link": "https://arxiv.org/pdf/2104.08724"}, "9": {"title": "Title:Affective Decoding for Empathetic Response Generation", "authors": "Authors:Chengkun Zheng, Guanyi Chen, Chenghua Lin, Ruizhe Li, Zhigang Chen", "submitted_date": "[Submitted on 18 Aug 2021 (v1), last revised 3 Sep 2021 (this version, v2)]", "abstract": "Abstract:  Understanding speaker's feelings and producing appropriate responses with\nemotion connection is a key communicative skill for empathetic dialogue\nsystems. In this paper, we propose a simple technique called Affective Decoding\nfor empathetic response generation. Our method can effectively incorporate\nemotion signals during each decoding step, and can additionally be augmented\nwith an auxiliary dual emotion encoder, which learns separate embeddings for\nthe speaker and listener given the emotion base of the dialogue. Extensive\nempirical studies show that our models are perceived to be more empathetic by\nhuman evaluations, in comparison to several strong mainstream methods for\nempathetic responding.", "pdf_link": "https://arxiv.org/pdf/2108.08102"}, "10": {"title": "Title:Where Are You? Localization from Embodied Dialog", "authors": "Authors:Meera Hahn, Jacob Krantz, Dhruv Batra, Devi Parikh, James M. Rehg, Stefan Lee, Peter Anderson", "submitted_date": "[Submitted on 16 Nov 2020 (v1), last revised 3 Sep 2021 (this version, v2)]", "abstract": "Abstract:  We present Where Are You? (WAY), a dataset of ~6k dialogs in which two humans\n-- an Observer and a Locator -- complete a cooperative localization task. The\nObserver is spawned at random in a 3D environment and can navigate from\nfirst-person views while answering questions from the Locator. The Locator must\nlocalize the Observer in a detailed top-down map by asking questions and giving\ninstructions. Based on this dataset, we define three challenging tasks:\nLocalization from Embodied Dialog or LED (localizing the Observer from dialog\nhistory), Embodied Visual Dialog (modeling the Observer), and Cooperative\nLocalization (modeling both agents). In this paper, we focus on the LED task --\nproviding a strong baseline model with detailed ablations characterizing both\ndataset biases and the importance of various modeling choices. Our best model\nachieves 32.7% success at identifying the Observer's location within 3m in\nunseen buildings, vs. 70.4% for human Locators.", "pdf_link": "https://arxiv.org/pdf/2011.08277"}}